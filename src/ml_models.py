# -*- coding: utf-8 -*-
"""ML_Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eap1wzXE1M4_gw1f09dtiosEbsTETvjE
"""

#  Copyright (c) 2023 DZX.
#
#  All rights reserved.
#
#  This software is protected by copyright law and international treaties. No part of this software may be reproduced, distributed, or transmitted in any form or by any means, including photocopying, recording, or other electronic or mechanical methods, without the prior written permission of the copyright owner.
#
#  For permission requests, please contact the copyright owner at the address below.
#
#  DZX
#
#  xindemicro@outlook.com
#

!nvidia-smi

"""# 导入库"""

from google.colab import drive
drive.mount('/content/drive')
import os
path = "/content/drive/MyDrive/"
os.chdir(path)

# Commented out IPython magic to ensure Python compatibility.
# Standard library imports
import csv
import matplotlib.pyplot as plt
import nltk
import numpy as np
import os
import pandas as pd
import pickle
import tensorflow as tf
import warnings

from keras.callbacks import Callback
from keras.layers import Conv1D, Dense, Embedding, Bidirectional, Dropout, GlobalAveragePooling1D, GlobalMaxPooling1D, Input, LSTM, MultiHeadAttention
from keras.models import Model, Sequential
from keras.optimizers import Adam
from keras.preprocessing.text import Tokenizer
from keras.regularizers import l2
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import class_weight
from tqdm import tqdm

from tensorflow.keras.layers import Conv1D, Dense, Embedding, Flatten, MaxPooling1D
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Input, Embedding, MultiHeadAttention, GlobalAveragePooling1D, Dropout, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences



# Jupyter-specific imports
# %matplotlib inline

# Download required NLTK resources
nltk.download('stopwords')
nltk.download('punkt')

# Suppress warnings
import warnings
warnings.filterwarnings("ignore")

# Define Porter Stemmer and Stop Words
porter_stemmer = PorterStemmer()
stop_words = list(stopwords.words("english"))

max_vocab=600000
max_len=1000

"""# 读入数据"""

# nrows_to_test = 1000
# while True:
#     try:
#         email_data = pd.read_csv("cleaned_email_06_full.csv", dtype={'email_id': object}, nrows=nrows_to_test, engine='python')
#         print(f"Successfully read {nrows_to_test} rows.")
#         nrows_to_test += 1000
#     except Exception as e:
#         print(f"Error encountered when trying to read {nrows_to_test} rows.")
#         print(e)
#         break

csv.field_size_limit(2**25)  # 提高字段大小限制
email_data = pd.read_csv("cleaned_email_06_full.csv", dtype={'email_id': object}, engine='python')
print(email_data.info())

email_data["spam"].value_counts()

def stem_tokenizer(text):
    words = [porter_stemmer.stem(token) for token in word_tokenize(text.lower())]
    return " ".join([w for w in words if w not in stop_words])

# 在处理之前，将 'from'、'to' 和 'body' 列组合为一个新列
email_data['combined'] = email_data['from'] + ' ' + email_data['to'] + ' ' + email_data['body']

"""## body处理"""

messages_body = []
for text in tqdm(email_data['body']):
  messages_body.append(stem_tokenizer(str(text)))

tokenizer_body = Tokenizer(num_words=max_vocab)
tokenizer_body.fit_on_texts(messages_body)
sequences_body = tokenizer_body.texts_to_sequences(messages_body)
word_index = tokenizer_body.word_index
data_body = pad_sequences(sequences_body, maxlen=max_len)

# 保存 messages
with open('message_body_stem_06.pickle', 'wb') as handle:
    pickle.dump(messages_body, handle, protocol=pickle.HIGHEST_PROTOCOL)
# 保存 data
with open('data_body_06.pickle', 'wb') as handle:
    pickle.dump(data_body, handle, protocol=pickle.HIGHEST_PROTOCOL)
# 保存 tokenizer
with open('vectokenizer_body.pickle', 'wb') as handle:
    pickle.dump(tokenizer_body, handle, protocol=pickle.HIGHEST_PROTOCOL)

"""## messages处理"""

messages = []
for text in tqdm(email_data['combined']):
  messages.append(stem_tokenizer(str(text)))

tokenizer = Tokenizer(num_words=max_vocab)
tokenizer.fit_on_texts(messages)
sequences = tokenizer.texts_to_sequences(messages)
word_index = tokenizer.word_index
data = pad_sequences(sequences, maxlen=max_len)



# 保存 messages
with open('message_stem_06.pickle', 'wb') as handle:
    pickle.dump(messages, handle, protocol=pickle.HIGHEST_PROTOCOL)
# 保存 data
with open('data_06.pickle', 'wb') as handle:
    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)
# 保存 tokenizer
with open('vectokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

# 保存 messages
with open('message_stem_06_max_len=1000.pickle', 'wb') as handle:
    pickle.dump(messages, handle, protocol=pickle.HIGHEST_PROTOCOL)
# 保存 data
with open('data_06_max_len=1000.pickle', 'wb') as handle:
    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)
# 保存 tokenizer
with open('vectokenizer_max_len=1000.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

"""## 读取数据"""

# 读取 messages
with open('message_stem_06.pickle', 'rb') as handle:
    messages = pickle.load(handle)
# 读取 data
with open('data_06.pickle', 'rb') as handle:
    data = pickle.load(handle)
# 读取 tokenizer
with open('vectokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

# # 读取 messages
# with open('message_body_stem_06.pickle', 'rb') as handle:
#     messages = pickle.load(handle)
# # 读取 data
# with open('data_body_06.pickle', 'rb') as handle:
#     data = pickle.load(handle)
# # 读取 tokenizer
# with open('vectokenizer_body.pickle', 'rb') as handle:
#     tokenizer = pickle.load(handle)

labels = pd.get_dummies(email_data["spam"]).values

def split_data(data, labels):
    return train_test_split(data, labels, shuffle=True, random_state=42, test_size=0.15)

"""# 回调函数"""

class CustomEarlyStopping(Callback):
    def __init__(self, monitor='val_loss', patience=10, min_delta=0, mode='auto', baseline=None, restore_best_weights=False, verbose=0, loss_threshold=0.01):
        super(CustomEarlyStopping, self).__init__()
        self.monitor = monitor
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.baseline = baseline
        self.restore_best_weights = restore_best_weights
        self.verbose = verbose
        self.wait = 0
        self.stopped_epoch = 0
        self.loss_threshold = loss_threshold
        self.best_weights = None

        if mode not in ['auto', 'min', 'max']:
            warnings.warn('EarlyStopping mode %s is unknown, '
                          'fallback to auto mode.' % (self.mode),
                          RuntimeWarning)
            self.mode = 'auto'

        if self.mode == 'min':
            self.monitor_op = np.less
            self.min_delta *= -1
            self.best = np.Inf
        elif self.mode == 'max':
            self.monitor_op = np.greater
            self.best = -np.Inf
        else:
            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):
                self.monitor_op = np.greater
                self.best = -np.Inf
            else:
                self.monitor_op = np.less
                self.best = np.Inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if current is None:
            warnings.warn('Early stopping conditioned on metric `%s` '
                          'which is not available. Available metrics are: %s' %
                          (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning)
            return

        if self.monitor_op(current - self.min_delta, self.best) and current > self.loss_threshold:
            self.best = current
            self.wait = 0
            if self.restore_best_weights:
                self.best_weights = self.model.get_weights()
        else:
            self.wait += 1
            if self.wait >= self.patience or current <= self.loss_threshold:
                self.stopped_epoch = epoch
                self.model.stop_training = True
                if self.restore_best_weights:
                    if self.verbose > 0:
                        print('Restoring model weights from the end of the best epoch.')
                    self.model.set_weights(self.best_weights)
                if self.verbose > 0:
                    print('Epoch %05d: early stopping' % (epoch + 1))
        
    def on_train_end(self, logs=None):
        if self.stopped_epoch > 0:
            print("Epoch %05d: early stopping" % (self.stopped_epoch + 1))

"""# LSTM"""

def build_and_train_lstm_model(x_train, y_train, x_val, y_val, embedding_mat_columns, name, batchsize=128):
    # 将 one-hot 编码的标签转换为二进制编码
    y_train_binary = np.argmax(y_train, axis=1)
    y_val_binary = np.argmax(y_val, axis=1)
    
    # 计算类别权重
    sample_weights = class_weight.compute_sample_weight('balanced', y_train_binary)
    
    model = Sequential()
    model.add(Embedding(input_dim=max_vocab, output_dim=embedding_mat_columns, input_length=max_len))
    model.add(LSTM(units=embedding_mat_columns // 4, kernel_regularizer=regularizers.l2(0.01), recurrent_regularizer=regularizers.l2(0.01)))
    model.add(Dropout(0.6))
    model.add(Dense(1, activation='sigmoid'))
    
    # 使用自定义学习率
    custom_lr = 0.0005
    optimizer = Adam(learning_rate=custom_lr)
    
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])
    
    early_stopping = CustomEarlyStopping(monitor='val_loss', patience=10, min_delta=0, mode='min', baseline=None, restore_best_weights=True, verbose=2, loss_threshold=0.01)
    result = model.fit(x_train, y_train_binary, epochs=50, batch_size=batchsize, validation_data=(x_val, y_val_binary), callbacks=[early_stopping], sample_weight=sample_weights)

    model_file = "./models/LSTM_model_" + name
    model.save(model_file)

    with open(f"./models/LSTM_result_{name}.p", "wb") as f:
        pickle.dump(result.history, f)

x_train, x_test, y_train, y_test = split_data(data, labels)

name = "message_body"
embedding_mat_columns = 128
batchsize = 128
build_and_train_lstm_model(x_train, y_train, x_test, y_test, embedding_mat_columns, name, batchsize)

name = "message_body"

with open("./models/LSTM_result_{}.p".format(name),"rb") as f:
    result=pickle.load(f)


history=result
plt.figure(figsize=(12,6))
plt.subplot(1, 2, 1)
plt.plot(history["loss"],label="Train loss")
plt.plot(history["val_loss"],label="Test loss")
plt.title('Loss')
plt.xlabel('epoch')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history["acc"],label="Train accuracy")
plt.plot(history["val_acc"],label="Test accuracy")
plt.title('Accuracy')
plt.xlabel('epoch')
plt.legend()

"""# LSTM_Glove"""

!wget https://nlp.stanford.edu/data/glove.6B.zip

!unzip glove.6B.zip

# 加载预训练词向量
def load_pretrained_embeddings(embedding_path, word_index, max_vocab, embedding_dim):
    embeddings_index = {}
    with open(embedding_path, encoding="utf-8") as f:
        for line in f:
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:], dtype="float32")
            embeddings_index[word] = coefs

    embedding_matrix = np.zeros((max_vocab, embedding_dim))
    for word, i in word_index.items():
        if i < max_vocab:
            embedding_vector = embeddings_index.get(word)
            if embedding_vector is not None:
                embedding_matrix[i] = embedding_vector

    return embedding_matrix

def build_and_train_lstm_glove_model(x_train, y_train, x_val, y_val, embedding_matrix, name, batchsize=128):
    # 将 one-hot 编码的标签转换为二进制编码
    y_train_binary = np.argmax(y_train, axis=1)
    y_val_binary = np.argmax(y_val, axis=1)
    
    # 计算类别权重
    sample_weights = class_weight.compute_sample_weight('balanced', y_train_binary)
    
    model = Sequential()
    model.add(Embedding(input_dim=max_vocab, output_dim=embedding_matrix.shape[1], input_length=max_len, weights=[embedding_matrix], trainable=False))
    model.add(LSTM(units=embedding_matrix.shape[1] // 4))
    model.add(Dropout(0.6))
    model.add(Dense(1, activation='sigmoid'))
    
    # 使用自定义学习率
    custom_lr = 0.0005
    optimizer = Adam(learning_rate=custom_lr)
    
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])

    early_stopping = CustomEarlyStopping(monitor='val_loss', patience=10, min_delta=0, mode='min', baseline=None, restore_best_weights=True, verbose=2, loss_threshold=0.01)
    result = model.fit(x_train, y_train_binary, epochs=50, batch_size=batchsize, validation_data=(x_val, y_val_binary), callbacks=[early_stopping], sample_weight=sample_weights)

    model_file = "./models/LSTM_model_" + name
    model.save(model_file)

    with open(f"./models/LSTM_result_{name}.p", "wb") as f:
        pickle.dump(result.history, f)

# 读取 data
with open('data_06.pickle', 'rb') as handle:
    data = pickle.load(handle)
x_train, x_test, y_train, y_test = split_data(data, labels)

# 加载预训练的GloVe词向量
embedding_path = "glove.6B.300d.txt"  
embedding_dim = 300  
embedding_matrix = load_pretrained_embeddings(embedding_path, tokenizer.word_index, max_vocab, embedding_dim)

name = "message_glove"
batchsize=128
build_and_train_lstm_glove_model(x_train, y_train, x_test, y_test, embedding_matrix, name, batchsize)

name = "message_body_glove"

with open("./models/LSTM_result_{}.p".format(name),"rb") as f:
    result=pickle.load(f)


history=result
plt.figure(figsize=(12,6))
plt.subplot(1, 2, 1)
plt.plot(history["loss"],label="Train loss")
plt.plot(history["val_loss"],label="Test loss")
plt.title('Loss')
plt.xlabel('epoch')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history["acc"],label="Train accuracy")
plt.plot(history["val_acc"],label="Test accuracy")
plt.title('Accuracy')
plt.xlabel('epoch')
plt.legend()

name = "message_glove"

with open("./models/LSTM_result_{}.p".format(name),"rb") as f:
    result=pickle.load(f)


history=result
plt.figure(figsize=(12,6))
plt.subplot(1, 2, 1)
plt.plot(history["loss"],label="Train loss")
plt.plot(history["val_loss"],label="Test loss")
plt.title('Loss')
plt.xlabel('epoch')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history["acc"],label="Train accuracy")
plt.plot(history["val_acc"],label="Test accuracy")
plt.title('Accuracy')
plt.xlabel('epoch')
plt.legend()

from tensorflow.keras.models import load_model
from sklearn.metrics import accuracy_score

name = "message_body_glove"

model_file = "./models/LSTM_model_" + name

# 从指定路径加载模型
model = load_model(model_file)

# 读取 data
with open('data_body_06.pickle', 'rb') as handle:
    data_body = pickle.load(handle)

x_train, x_test, y_train, y_test = split_data(data_body, labels)
y_pred = model.predict(x_test)
y_pred = np.round(np.squeeze(y_pred)).astype(int)
y_true = np.argmax(y_test, axis=1)
accuracy = accuracy_score(y_true, y_pred)
print("Accuracy: %.2f%%" % (accuracy*100))

from tensorflow.keras.models import load_model
from sklearn.metrics import accuracy_score

name = "message_glove"

model_file = "./models/LSTM_model_" + name

# 从指定路径加载模型
model = load_model(model_file)

# 读取 data
with open('data_06.pickle', 'rb') as handle:
    data = pickle.load(handle)

x_train, x_test, y_train, y_test = split_data(data, labels)
y_pred = model.predict(x_test)
y_pred = np.round(np.squeeze(y_pred)).astype(int)
y_true = np.argmax(y_test, axis=1)
accuracy = accuracy_score(y_true, y_pred)
print("Accuracy: %.2f%%" % (accuracy*100))

"""# 1D-CNN"""

def build_and_train_cnn_model(x_train, y_train, x_val, y_val, embedding_mat_columns, name, batchsize=128):
    y_train_binary = np.argmax(y_train, axis=1)
    y_val_binary = np.argmax(y_val, axis=1)
    
    sample_weights = class_weight.compute_sample_weight('balanced', y_train_binary)
    
    model = Sequential()
    model.add(Embedding(input_dim=max_vocab, output_dim=embedding_mat_columns, input_length=max_len))
    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))
    model.add(GlobalMaxPooling1D())
    model.add(Dense(64, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))

    optimizer = Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])

    early_stopping = CustomEarlyStopping(monitor='val_loss', patience=10, min_delta=0, mode='min', baseline=None, restore_best_weights=True, verbose=2, loss_threshold=0.01)
    result = model.fit(x_train, y_train_binary, epochs=50, batch_size=batchsize, validation_data=(x_val, y_val_binary), callbacks=[early_stopping], sample_weight=sample_weights)

    model_file = "./models/CNN_model_" + name
    model.save(model_file)

    with open(f"./models/CNN_result_{name}.p", "wb") as f:
        pickle.dump(result.history, f)

x_train, x_test, y_train, y_test = split_data(data, labels)

name = "message_body"
embedding_mat_columns = 128
batchsize = 128
build_and_train_cnn_model(x_train, y_train, x_test, y_test, embedding_mat_columns, name, batchsize)

name = "message_body"

with open("./models/CNN_result_{}.p".format(name),"rb") as f:
    result=pickle.load(f)


history=result
plt.figure(figsize=(12,6))
plt.subplot(1, 2, 1)
plt.plot(history["loss"],label="Train loss")
plt.plot(history["val_loss"],label="Test loss")
plt.title('Loss')
plt.xlabel('epoch')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history["acc"],label="Train accuracy")
plt.plot(history["val_acc"],label="Test accuracy")
plt.title('Accuracy')
plt.xlabel('epoch')
plt.legend()

"""# 1D-CNN_Glove"""

def load_glove_embeddings(glove_file, word_index, embedding_dim):
    embeddings_index = {}
    with open(glove_file, 'r', encoding='utf-8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = coefs

    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))
    for word, i in word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

    return embedding_matrix

from tensorflow.keras.layers import Dropout

def build_and_train_cnn_glove_model(x_train, y_train, x_val, y_val, embedding_matrix, name, batchsize=128):
    y_train_binary = np.argmax(y_train, axis=1)
    y_val_binary = np.argmax(y_val, axis=1)
    
    sample_weights = class_weight.compute_sample_weight('balanced', y_train_binary)

    model = Sequential()
    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False))
    model.add(Conv1D(64, 5, activation='relu'))
    model.add(MaxPooling1D(5))
    model.add(Conv1D(64, 5, activation='relu'))
    model.add(MaxPooling1D(5))
    model.add(Flatten())
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))

    optimizer = Adam(learning_rate=0.0005)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])

    early_stopping = CustomEarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, loss_threshold=0.01, restore_best_weights=True)
    result = model.fit(x_train, y_train_binary, epochs=50, batch_size=batchsize, validation_data=(x_val, y_val_binary), callbacks=[early_stopping], sample_weight=sample_weights)

    model_file = "./models/CNN_model_" + name
    model.save(model_file)

    with open(f"./models/CNN_result_{name}.p", "wb") as f:
        pickle.dump(result.history, f)

x_train, x_test, y_train, y_test = split_data(data, labels)

glove_file = "glove.6B.300d.txt"  
embedding_dim = 300
embedding_matrix = load_glove_embeddings(glove_file, tokenizer.word_index, embedding_dim)
name = "message_body_glove"
batchsize=128
build_and_train_cnn_glove_model(x_train, y_train, x_test, y_test, embedding_matrix, name, batchsize)

name = "message_body_glove"

with open("./models/CNN_result_{}.p".format(name),"rb") as f:
    result=pickle.load(f)


history=result
plt.figure(figsize=(12,6))
plt.subplot(1, 2, 1)
plt.plot(history["loss"],label="Train loss")
plt.plot(history["val_loss"],label="Test loss")
plt.title('Loss')
plt.xlabel('epoch')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history["acc"],label="Train accuracy")
plt.plot(history["val_acc"],label="Test accuracy")
plt.title('Accuracy')
plt.xlabel('epoch')
plt.legend()

"""# 基于Transformer（内存不足）"""

# def build_and_train_transformer_model(x_train, y_train, x_val, y_val, embedding_mat_columns, name, batchsize=128):
#     y_train_binary = np.argmax(y_train, axis=1)
#     y_val_binary = np.argmax(y_val, axis=1)
    
#     sample_weights = class_weight.compute_sample_weight('balanced', y_train_binary)

#     inputs = Input(shape=(max_len,))
#     x = Embedding(input_dim=max_vocab, output_dim=embedding_mat_columns, input_length=max_len)(inputs)
#     x = MultiHeadAttention(num_heads=8, key_dim=embedding_mat_columns)(x, x, x)
#     x = GlobalAveragePooling1D()(x)
#     x = Dense(64, activation='relu')(x)
#     outputs = Dense(1, activation='sigmoid')(x)

#     model = Model(inputs=inputs, outputs=outputs)

#     optimizer = Adam(learning_rate=0.001)
#     model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])

#     early_stopping = CustomEarlyStopping(monitor='val_loss', patience=10, min_delta=0, mode='min', baseline=None, restore_best_weights=True, verbose=2, loss_threshold=0.01)
#     result = model.fit(x_train, y_train_binary, epochs=50, batch_size=batchsize, validation_data=(x_val, y_val_binary), callbacks=[early_stopping], sample_weight=sample_weights)

#     model_file = "./models/Transformer_model_" + name
#     model.save(model_file)

#     with open(f"./models/Transformer_result_{name}.p", "wb") as f:
#         pickle.dump(result.history, f)

# x_train, x_test, y_train, y_test = split_data(data, labels)

# name = "message_body"
# embedding_mat_columns = 128
# batchsize = 128
# build_and_train_transformer_model(x_train, y_train, x_test, y_test, embedding_mat_columns, name, batchsize)

"""# 基于Transformer_Glove

"""

def load_glove_embeddings(glove_file, word_index, embedding_dim):
    embeddings_index = {}
    with open(glove_file, 'r', encoding='utf-8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = coefs

    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))
    for word, i in word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

    return embedding_matrix

glove_file = "glove.6B.100d.txt"  
embedding_dim = 100
embedding_matrix = load_glove_embeddings(glove_file, tokenizer.word_index, embedding_dim)

def build_and_train_transformer_glove_model(x_train, y_train, x_val, y_val, embedding_matrix, name, batchsize=32):
    y_train_binary = np.argmax(y_train, axis=1)
    y_val_binary = np.argmax(y_val, axis=1)

    sample_weights = class_weight.compute_sample_weight('balanced', y_train_binary)

    input_layer = Input(shape=(max_len,))
    embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)(input_layer)
    attention_layer = MultiHeadAttention(num_heads=4, key_dim=embedding_dim // 4, dropout=0.1)(embedding_layer, embedding_layer)
    pooling_layer = GlobalAveragePooling1D()(attention_layer)
    dropout_layer = Dropout(0.5)(pooling_layer)
    output_layer = Dense(1, activation='sigmoid')(dropout_layer)

    model = Model(inputs=input_layer, outputs=output_layer)

    optimizer = Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])

    early_stopping = CustomEarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, loss_threshold=0.01, restore_best_weights=True)
    result = model.fit(x_train, y_train_binary, epochs=50, batch_size=batchsize, validation_data=(x_val, y_val_binary), callbacks=[early_stopping], sample_weight=sample_weights)

    model_file = "./models/Transformer_model_" + name
    model.save(model_file)

    with open(f"./models/Transformer_result_{name}.p", "wb") as f:
        pickle.dump(result.history, f)

name = "message_glove"
batchsize = 32
x_train, x_test, y_train, y_test = split_data(data, labels)
build_and_train_transformer_glove_model(x_train, y_train, x_test, y_test, embedding_matrix, name, batchsize)

name = "message_glove"

with open("./models/Transformer_result_{}.p".format(name),"rb") as f:
    result=pickle.load(f)


history=result
plt.figure(figsize=(12,6))
plt.subplot(1, 2, 1)
plt.plot(history["loss"],label="Train loss")
plt.plot(history["val_loss"],label="Test loss")
plt.title('Loss')
plt.xlabel('epoch')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history["acc"],label="Train accuracy")
plt.plot(history["val_acc"],label="Test accuracy")
plt.title('Accuracy')
plt.xlabel('epoch')
plt.legend()

from tensorflow.keras.models import load_model
from sklearn.metrics import accuracy_score

name = "message_glove"

model_file = "./models/Transformer_model_" + name

# 从指定路径加载模型
model = load_model(model_file)

# 读取 data
with open('data_06_max_len=1000.pickle', 'rb') as handle:
    data = pickle.load(handle)

x_train, x_test, y_train, y_test = split_data(data, labels)
y_pred = model.predict(x_test)
y_pred = np.round(np.squeeze(y_pred)).astype(int)
y_true = np.argmax(y_test, axis=1)
accuracy = accuracy_score(y_true, y_pred)
print("Accuracy: %.2f%%" % (accuracy*100))

"""## 内存不足"""

def build_and_train_transformer_glove_model(x_train, y_train, x_val, y_val, embedding_matrix, name, batchsize=128):
    y_train_binary = np.argmax(y_train, axis=1)
    y_val_binary = np.argmax(y_val, axis=1)

    sample_weights = class_weight.compute_sample_weight('balanced', y_train_binary)

    input_layer = Input(shape=(max_len,))
    embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)(input_layer)
    attention_layer = MultiHeadAttention(num_heads=8, key_dim=embedding_dim // 8, dropout=0.1)(embedding_layer, embedding_layer)
    pooling_layer = GlobalAveragePooling1D()(attention_layer)
    dropout_layer = Dropout(0.5)(pooling_layer)
    output_layer = Dense(1, activation='sigmoid')(dropout_layer)

    model = Model(inputs=input_layer, outputs=output_layer)

    optimizer = Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])

    early_stopping = CustomEarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, loss_threshold=0.01, restore_best_weights=True)
    result = model.fit(x_train, y_train_binary, epochs=50, batch_size=batchsize, validation_data=(x_val, y_val_binary), callbacks=[early_stopping], sample_weight=sample_weights)

    model_file = "./models/Transformer_model_" + name
    model.save(model_file)

    with open(f"./models/Transformer_result_{name}.p", "wb") as f:
        pickle.dump(result.history, f)

x_train, x_test, y_train, y_test = split_data(data, labels)

name = "message_body_glove"
batchsize=64
build_and_train_transformer_glove_model(x_train, y_train, x_test, y_test, embedding_matrix, name, batchsize)

"""# LSTM_BERT（内存不足）"""

# print(x_train)
# print(y_train)
# print(x_test)
# print(y_test)

!pip install transformers

import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

def build_and_train_bert_classifier(x_train_raw, y_train, x_test_raw, y_test, model_name, batchsize=32):
    max_len = 128  # 根据您的实际情况调整此值

    # 使用预训练的 BERT 分词器对输入进行编码
    tokenizer = BertTokenizer.from_pretrained(model_name)
    x_train_encoded = tokenizer(x_train_raw, padding=True, truncation=True, max_length=max_len, return_tensors="tf")
    x_test_encoded = tokenizer(x_test_raw, padding=True, truncation=True, max_length=max_len, return_tensors="tf")

    # 构建分类器
    bert_model = TFBertModel.from_pretrained(model_name)
    input_layer = Input(shape=(max_len,), dtype=tf.int32)
    bert_output = bert_model(input_layer)
    pooled_output = bert_output.pooler_output  # 获取 BERT 模型的池化输出
    dropout_layer = Dropout(0.1)(pooled_output)
    output_layer = Dense(1, activation='sigmoid')(dropout_layer)

    # 创建和编译模型
    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['acc'])

    # 训练模型
    early_stopping = CustomEarlyStopping(monitor='val_loss', patience=10, min_delta=0, mode='min', baseline=None, restore_best_weights=True, verbose=2, loss_threshold=0.01)
    result = model.fit(x_train_encoded['input_ids'], y_train, epochs=50, batch_size=batchsize, validation_data=(x_test_encoded['input_ids'], y_test), callbacks=[early_stopping])

    model_file = "./models/BERT_model_" + name
    model.save(model_file)

    with open(f"./models/BERT_result_{name}.p", "wb") as f:
        pickle.dump(result.history, f)

# 使用原始文本数据拆分训练集和测试集
X = np.array(messages)
y = np.argmax(labels, axis=1)
x_train_raw, x_test_raw, y_train_binary, y_test_binary = train_test_split(X, y, test_size=0.3, random_state=42)

# 使用预训练的 BERT 模型
model_name = "bert-base-uncased"
name = "message_body_bert"
batchsize = 32
build_and_train_bert_classifier(x_train_raw, y_train_binary, x_test_raw, y_test_binary, model_name, batchsize)

import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

def build_and_train_bert_classifier(x_train_raw, y_train, x_test_raw, y_test, model_name, batchsize=16):
    max_len = 32  # 根据您的实际情况调整此值

    # 使用预训练的 BERT 分词器对输入进行编码
    tokenizer = BertTokenizer.from_pretrained(model_name)

    # 使用 tf.data.Dataset 进行数据流式传输
    train_dataset = tf.data.Dataset.from_tensor_slices((x_train_raw, y_train)).map(lambda x, y: (tokenizer(x, padding=True, truncation=True, max_length=max_len, return_tensors="tf"), y)).batch(batchsize)
    test_dataset = tf.data.Dataset.from_tensor_slices((x_test_raw, y_test)).map(lambda x, y: (tokenizer(x, padding=True, truncation=True, max_length=max_len, return_tensors="tf"), y)).batch(batchsize)

    # 构建分类器
    bert_model = TFBertModel.from_pretrained(model_name)
    input_layer = Input(shape=(max_len,), dtype=tf.int32)
    bert_output = bert_model(input_layer)
    pooled_output = bert_output.pooler_output  # 获取 BERT 模型的池化输出
    dropout_layer = Dropout(0.1)(pooled_output)
    output_layer = Dense(1, activation='sigmoid')(dropout_layer)

    # 创建和编译模型
    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['acc'])

    # 训练模型
    early_stopping = CustomEarlyStopping(monitor='val_loss', patience=10, min_delta=0, mode='min', baseline=None, restore_best_weights=True, verbose=2, loss_threshold=0.01)
    result = model.fit(train_dataset, epochs=50, validation_data=test_dataset, callbacks=[early_stopping])

    model_file = "./models/BERT_model_" + name
    model.save(model_file)

    with open(f"./models/BERT_result_{name}.p", "wb") as f:
        pickle.dump(result.history, f)

# 使用原始文本数据拆分训练集和测试集
X = np.array(messages)
y = np.argmax(labels, axis=1)
x_train_raw, x_test_raw, y_train_binary, y_test_binary = train_test_split(X, y, test_size=0.3, random_state=42)

# 使用较小的预训练 BERT 模型
model_name = "distilbert-base-uncased"
name = "message_body_distilbert"
batchsize = 16
build_and_train_bert_classifier(x_train_raw, y_train_binary, x_test_raw, y_test_binary, model_name, batchsize)

import numpy as np
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertModel
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

strategy = tf.distribute.OneDeviceStrategy("GPU:0")

def build_and_train_bert_classifier(x_train_raw, y_train, x_test_raw, y_test, model_name, batchsize=8):
    max_len = 32

    tokenizer = DistilBertTokenizer.from_pretrained(model_name)

    train_dataset = tf.data.Dataset.from_tensor_slices((x_train_raw, y_train)).map(lambda x, y: (tokenizer(x, padding=True, truncation=True, max_length=max_len, return_tensors="tf"), y)).batch(batchsize)
    test_dataset = tf.data.Dataset.from_tensor_slices((x_test_raw, y_test)).map(lambda x, y: (tokenizer(x, padding=True, truncation=True, max_length=max_len, return_tensors="tf"), y)).batch(batchsize)

    bert_model = TFDistilBertModel.from_pretrained(model_name)
    input_layer = Input(shape=(max_len,), dtype=tf.int32)
    bert_output = bert_model(input_layer)
    pooled_output = bert_output.pooler_output
    dropout_layer = Dropout(0.1)(pooled_output)
    output_layer = Dense(1, activation='sigmoid')(dropout_layer)

    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['acc'])

    early_stopping = CustomEarlyStopping(monitor='val_loss', patience=10, min_delta=0, mode='min', baseline=None, restore_best_weights=True, verbose=2, loss_threshold=0.01)
    result = model.fit(train_dataset, epochs=50, validation_data=test_dataset, callbacks=[early_stopping])

    model_file = "./models/DistilBERT_model_" + name
    model.save(model_file)

    with open(f"./models/DistilBERT_result_{name}.p", "wb") as f:
        pickle.dump(result.history, f)

X = np.array(messages)
y = np.argmax(labels, axis=1)
x_train_raw, x_test_raw, y_train_binary, y_test_binary = train_test_split(X, y, test_size=0.3, random_state=42)

model_name = "distilbert-base-uncased"
name = "message_body_distilbert"
batchsize = 8

with strategy.scope():
    build_and_train_bert_classifier(x_train_raw, y_train_binary, x_test_raw, y_test_binary, model_name, batchsize)

import numpy as np
import tensorflow as tf
from transformers import ElectraTokenizer, TFElectraModel
from sklearn.model_selection import train_test_split, KFold
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import mixed_precision

strategy = tf.distribute.OneDeviceStrategy("GPU:0")

policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

def build_and_train_electra_classifier(x_train_raw, y_train, x_test_raw, y_test, model_name, batchsize=4):
    max_len = 32

    tokenizer = ElectraTokenizer.from_pretrained(model_name)

    train_dataset = tf.data.Dataset.from_tensor_slices((x_train_raw, y_train)).map(lambda x, y: (tokenizer(x, padding=True, truncation=True, max_length=max_len, return_tensors="tf"), y)).batch(batchsize)
    test_dataset = tf.data.Dataset.from_tensor_slices((x_test_raw, y_test)).map(lambda x, y: (tokenizer(x, padding=True, truncation=True, max_length=max_len, return_tensors="tf"), y)).batch(batchsize)

    electra_model = TFElectraModel.from_pretrained(model_name)
    input_layer = Input(shape=(max_len,), dtype=tf.int32)
    electra_output = electra_model(input_layer)
    pooled_output = electra_output.pooler_output
    dropout_layer = Dropout(0.1)(pooled_output)
    output_layer = Dense(1, activation='sigmoid', dtype=tf.float32)(dropout_layer)

    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['acc'])

    early_stopping = CustomEarlyStopping(monitor='val_loss', patience=10, min_delta=0, mode='min', baseline=None, restore_best_weights=True, verbose=2, loss_threshold=0.01)
    result = model.fit(train_dataset, epochs=50, validation_data=test_dataset, callbacks=[early_stopping])

    return model

X = np.array(messages)
y = np.argmax(labels, axis=1)

model_name = "google/electra-small-discriminator"
name = "message_body_electra"
batchsize = 4

kf = KFold(n_splits=5) # Adjust the number of splits based on available GPU memory
i = 1
for train_index, test_index in kf.split(X):
    x_train_raw, x_test_raw = X[train_index], X[test_index]
    y_train_binary, y_test_binary = y[train_index], y[test_index]

    with strategy.scope():
        model = build_and_train_electra_classifier(x_train_raw, y_train_binary, x_test_raw, y_test_binary, model_name, batchsize)

    model_file = f"./models/Electra_model_{name}_fold_{i}.h5"
    model.save(model_file)
    i += 1

import numpy as np
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertModel
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import mixed_precision

strategy = tf.distribute.OneDeviceStrategy("GPU:0")

policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

def build_and_train_distilbert_classifier(x_train_raw, y_train, x_test_raw, y_test, model_name, batchsize=4):
    max_len = 32

    tokenizer = DistilBertTokenizer.from_pretrained(model_name)

    train_dataset = tf.data.Dataset.from_tensor_slices((x_train_raw, y_train)).map(lambda x, y: (tokenizer(x, padding=True, truncation=True, max_length=max_len, return_tensors="tf"), y)).batch(batchsize)
    test_dataset = tf.data.Dataset.from_tensor_slices((x_test_raw, y_test)).map(lambda x, y: (tokenizer(x, padding=True, truncation=True, max_length=max_len, return_tensors="tf"), y)).batch(batchsize)

    distilbert_model = TFDistilBertModel.from_pretrained(model_name)
    input_layer = Input(shape=(max_len,), dtype=tf.int32)
    distilbert_output = distilbert_model(input_layer)
    pooled_output = distilbert_output.last_hidden_state[:, 0]
    dropout_layer = Dropout(0.1)(pooled_output)
    output_layer = Dense(1, activation='sigmoid', dtype=tf.float32)(dropout_layer)

    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['acc'])

    early_stopping = CustomEarlyStopping(monitor='val_loss', patience=10, min_delta=0, mode='min', baseline=None, restore_best_weights=True, verbose=2, loss_threshold=0.01)
    result = model.fit(train_dataset, epochs=50, validation_data=test_dataset, callbacks=[early_stopping])

    return model

X = np.array(messages)
y = np.argmax(labels, axis=1)
x_train_raw, x_test_raw, y_train_binary, y_test_binary = train_test_split(X, y, test_size=0.3, random_state=42)

model_name = "distilbert-base-uncased"
name = "message_body_distilbert"
batchsize = 4

with strategy.scope():
    model = build_and_train_distilbert_classifier(x_train_raw, y_train_binary, x_test_raw, y_test_binary, model_name, batchsize)

model_file = f"./models/DistilBert_model_{name}.h5"
model.save(model_file)

import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import mixed_precision

strategy = tf.distribute.OneDeviceStrategy("GPU:0")

policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

def build_and_train_tinybert_classifier(x_train_raw, y_train, x_test_raw, y_test, model_name, batchsize=4):
    max_len = 32

    tokenizer = BertTokenizer.from_pretrained(model_name)

    train_dataset = tf.data.Dataset.from_tensor_slices((x_train_raw, y_train)).map(lambda x, y: (tokenizer(x, padding=True, truncation=True, max_length=max_len, return_tensors="tf"), y)).batch(batchsize)
    test_dataset = tf.data.Dataset.from_tensor_slices((x_test_raw, y_test)).map(lambda x, y: (tokenizer(x, padding=True, truncation=True, max_length=max_len, return_tensors="tf"), y)).batch(batchsize)

    tinybert_model = TFBertModel.from_pretrained(model_name)
    input_layer = Input(shape=(max_len,), dtype=tf.int32)
    tinybert_output = tinybert_model(input_layer)
    pooled_output = tinybert_output.pooler_output
    dropout_layer = Dropout(0.1)(pooled_output)
    output_layer = Dense(1, activation='sigmoid', dtype=tf.float32)(dropout_layer)

    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['acc'])

    early_stopping = CustomEarlyStopping(monitor='val_loss', patience=10, min_delta=0, mode='min', baseline=None, restore_best_weights=True, verbose=2, loss_threshold=0.01)
    result = model.fit(train_dataset, epochs=50, validation_data=test_dataset, callbacks=[early_stopping])

    return model

X = np.array(messages)
y = np.argmax(labels, axis=1)
x_train_raw, x_test_raw, y_train_binary, y_test_binary = train_test_split(X, y, test_size=0.3, random_state=42)

model_name = "prajjwal1/tinybert"
name = "message_body_tinybert"
batchsize = 4

with strategy.scope():
    model = build_and_train_tinybert_classifier(x_train_raw, y_train_binary, x_test_raw, y_test_binary, model_name, batchsize)

model_file = f"./models/TinyBert_model_{name}.h5"
model.save(model_file)

import numpy as np
import tensorflow as tf
from transformers import MobileBertTokenizer, TFMobileBertModel
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

def build_and_train_mobilebert_classifier(x_train_raw, y_train, x_test_raw, y_test, model_name, batchsize=4):
    max_len = 32

    tokenizer = MobileBertTokenizer.from_pretrained(model_name)

    train_dataset = tf.data.Dataset.from_tensor_slices((x_train_raw, y_train)).map(lambda x, y: (tokenizer(x, padding=True, truncation=True, max_length=max_len, return_tensors="tf"), y)).batch(batchsize)
    test_dataset = tf.data.Dataset.from_tensor_slices((x_test_raw, y_test)).map(lambda x, y: (tokenizer(x, padding=True, truncation=True, max_length=max_len, return_tensors="tf"), y)).batch(batchsize)

    mobilebert_model = TFMobileBertModel.from_pretrained(model_name)
    input_layer = Input(shape=(max_len,), dtype=tf.int32)
    mobilebert_output = mobilebert_model(input_layer)
    pooled_output = mobilebert_output.pooler_output
    dropout_layer = Dropout(0.1)(pooled_output)
    output_layer = Dense(1, activation='sigmoid')(dropout_layer)

    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['acc'])

    early_stopping = CustomEarlyStopping(monitor='val_loss', patience=10, min_delta=0, mode='min', baseline=None, restore_best_weights=True, verbose=2, loss_threshold=0.01)
    result = model.fit(train_dataset, epochs=50, validation_data=test_dataset, callbacks=[early_stopping])

    return model

X = np.array(messages)
y = np.argmax(labels, axis=1)
x_train_raw, x_test_raw, y_train_binary, y_test_binary = train_test_split(X, y, test_size=0.3, random_state=42)

model_name = "google/mobilebert-uncased"
name = "message_body_mobilebert"
batchsize = 4

with strategy.scope():
    model = build_and_train_mobilebert_classifier(x_train_raw, y_train_binary, x_test_raw, y_test_binary, model_name, batchsize)

model_file = f"./models/MobileBert_model_{name}.h5"
model.save(model_file)

import tensorflow as tf
from transformers import MobileBertTokenizer, TFAutoModel

def train_step(inputs, labels, model, optimizer, loss_object):
    with tf.GradientTape() as tape:
        predictions = model(inputs, training=True)
        loss = loss_object(labels, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss

def train_model(x_train, y_train, x_test, y_test, model_name, batch_size=4, epochs=1, max_len=32):
    tokenizer = MobileBertTokenizer.from_pretrained(model_name)
    model = TFAutoModel.from_pretrained(model_name)

    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)
    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)

    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)
    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)

    for epoch in range(epochs):
        print(f"Epoch: {epoch+1}/{epochs}")

        # Training
        batch = 0
        for inputs, labels in train_dataset:
            tokenized_inputs = tokenizer(list(inputs.numpy()), padding=True, truncation=True, max_length=max_len, return_tensors="tf")
            loss = train_step(tokenized_inputs, labels, model, optimizer, loss_object)
            if batch % 10 == 0:
                print(f"Batch: {batch}, Loss: {loss.numpy()}")
            batch += 1

        # Evaluation
        test_loss = 0
        test_batches = 0
        for inputs, labels in test_dataset:
            tokenized_inputs = tokenizer(list(inputs.numpy()), padding=True, truncation=True, max_length=max_len, return_tensors="tf")
            predictions = model(tokenized_inputs, training=False)
            loss = loss_object(labels, predictions)
            test_loss += loss
            test_batches += 1
        print(f"Test Loss: {test_loss/test_batches}")
        
X = np.array(messages)
y = np.argmax(labels, axis=1)
x_train_raw, x_test_raw, y_train_binary, y_test_binary = train_test_split(X, y, test_size=0.3, random_state=42)
train_model(x_train_raw, y_train_binary, x_test_raw, y_test_binary, "google/mobilebert-uncased", batch_size=4, epochs=1, max_len=32)