# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/127Zv1EfslC5A29n_fE7osDjwR3dCYdvS
"""

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')
import os
path = "/content/drive/MyDrive/"
os.chdir(path)

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, LSTM, SpatialDropout1D
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.models import load_model
import matplotlib.pyplot as plt
import csv

csv.field_size_limit(2**25)  # 提高字段大小限制
email_data = pd.read_csv("cleaned_email_06_full.csv", dtype={'email_id': object}, engine='python')
print(email_data.info())
labels = pd.get_dummies(email_data["spam"]).values
email_data['structure'] = email_data['parts'].astype(str) + email_data['attachments'].astype(str) + email_data['html'].astype(str) + email_data['links'].astype(str)
email_data['combined_strsub'] = email_data['structure'] + email_data['subject']
email_data['combined_strbod'] = email_data['structure'] + email_data['body']
email_data['combined_subbod'] = email_data['subject'] + email_data['body']
email_data['combined'] = email_data['structure'] + email_data['subject'] + email_data['body']
email_data.info()



# 加载数据
df = email_data

# 设定词汇表的大小
num_words = 5000
# 设定每封邮件的最大长度（如果邮件超过此长度，则将其截断，如果不足，则用0填充）
max_len = 500

# 初始化一个字典来存储每次运行的结果
results = {}

features = ['structure', 'subject', 'body', 'combined_strsub', 'combined_strbod', 'combined_subbod', 'combined']

for feature in features:
    print(f'Running for content: {feature}')

    df['content'] = df[feature]

    # 然后将分类标签转换为二进制格式
    encoder = LabelEncoder()
    df['spam'] = encoder.fit_transform(df['spam'])

    tokenizer = Tokenizer(num_words=num_words, split=' ')
    tokenizer.fit_on_texts(df['content'].values)

    X = tokenizer.texts_to_sequences(df['content'].values)
    print(X)
    print(len(X))
    X = pad_sequences(X, maxlen=max_len)
    print(X)
    print(X.shape)
    Y = pd.get_dummies(df['spam']).values
    print(Y)
    print(Y.shape)
    break;

    model = Sequential()
    model.add(Embedding(num_words, 128, input_length=max_len))
    model.add(SpatialDropout1D(0.4))  # 在 LSTM 层之前使用 Dropout
    model.add(LSTM(196, dropout=0.2, recurrent_dropout=0))  # 设置 recurrent_dropout=0
    model.add(Dense(2, activation='softmax'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)

    # 创建 ModelCheckpoint 和 EarlyStopping 回调函数
    checkpoint = ModelCheckpoint(f'lstm_{feature}_best_model.h5', verbose=1, monitor='val_loss', save_best_only=True,
                                 mode='auto')
    earlystop = EarlyStopping(monitor='val_loss', patience=3)

    # 在 fit 函数中使用这些回调函数
    history = model.fit(X_train, Y_train, epochs=10, batch_size=32, verbose=1, validation_data=(X_val, Y_val),
                        callbacks=[checkpoint, earlystop])
    # 绘制训练损失和验证损失
    plt.figure(figsize=(12,6))
    plt.subplot(1, 2, 1)
    plt.plot(history.history["loss"],label="Train loss")
    plt.plot(history.history["val_loss"],label="Test loss", linestyle='--')
    plt.title('Loss')
    plt.xlabel('epoch')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history["accuracy"],label="Train accuracy")
    plt.plot(history.history["val_accuracy"],label="Test accuracy", linestyle='--')
    plt.title('Accuracy')
    plt.xlabel('epoch')
    plt.legend()
    plt.show()

    # 计算在验证集上的评分和精确度
    score, acc = model.evaluate(X_val, Y_val, verbose=1, batch_size=32)

    # 在验证集上进行预测
    Y_pred = model.predict(X_val)

    # 将预测结果转换为类别
    Y_pred_classes = np.argmax(Y_pred, axis=1)
    Y_val_classes = np.argmax(Y_val, axis=1)

    # 计算精确率，召回率和 F1 分数
    precision = precision_score(Y_val_classes, Y_pred_classes)
    recall = recall_score(Y_val_classes, Y_pred_classes)
    f1 = f1_score(Y_val_classes, Y_pred_classes)

    print(f'For {feature}:\n Precision: {precision}, Recall: {recall}, F1 Score: {f1}, Accuracy: {acc}\n')

    # 将这次运行的结果存入字典
    results[feature] = {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'accuracy': acc
    }

# 转换 results 字典为 DataFrame，并保存到 CSV 文件
results_df = pd.DataFrame(results).T
results_df.to_csv('lstm_results.csv')



"""# LSTM"""



# 加载数据
df = email_data

# 设定词汇表的大小
num_words = 5000
# 设定每封邮件的最大长度（如果邮件超过此长度，则将其截断，如果不足，则用0填充）
max_len = 500

# 初始化一个字典来存储每次运行的结果
results = {}

features = ['structure', 'subject', 'body', 'combined_strsub', 'combined_strbod', 'combined_subbod', 'combined']

for feature in features:
    print(f'Running for content: {feature}')

    df['content'] = df[feature]

    # 然后将分类标签转换为二进制格式
    encoder = LabelEncoder()
    df['spam'] = encoder.fit_transform(df['spam'])

    tokenizer = Tokenizer(num_words=num_words, split=' ')
    tokenizer.fit_on_texts(df['content'].values)

    X = tokenizer.texts_to_sequences(df['content'].values)
    X = pad_sequences(X, maxlen=max_len)
    Y = pd.get_dummies(df['spam']).values

    model = Sequential()
    model.add(Embedding(num_words, 128, input_length=max_len))
    model.add(SpatialDropout1D(0.4))  # 在 LSTM 层之前使用 Dropout
    model.add(LSTM(196, dropout=0.2, recurrent_dropout=0))  # 设置 recurrent_dropout=0
    model.add(Dense(2, activation='softmax'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)

    # 创建 ModelCheckpoint 和 EarlyStopping 回调函数
    checkpoint = ModelCheckpoint(f'lstm_{feature}_best_model.h5', verbose=1, monitor='val_loss', save_best_only=True,
                                 mode='auto')
    earlystop = EarlyStopping(monitor='val_loss', patience=3)

    # 在 fit 函数中使用这些回调函数
    history = model.fit(X_train, Y_train, epochs=10, batch_size=32, verbose=1, validation_data=(X_val, Y_val),
                        callbacks=[checkpoint, earlystop])
    # 绘制训练损失和验证损失
    plt.figure(figsize=(12,6))
    plt.subplot(1, 2, 1)
    plt.plot(history.history["loss"],label="Train loss")
    plt.plot(history.history["val_loss"],label="Test loss", linestyle='--')
    plt.title('Loss')
    plt.xlabel('epoch')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history["accuracy"],label="Train accuracy")
    plt.plot(history.history["val_accuracy"],label="Test accuracy", linestyle='--')
    plt.title('Accuracy')
    plt.xlabel('epoch')
    plt.legend()
    plt.show()

    # 计算在验证集上的评分和精确度
    score, acc = model.evaluate(X_val, Y_val, verbose=1, batch_size=32)

    # 在验证集上进行预测
    Y_pred = model.predict(X_val)

    # 将预测结果转换为类别
    Y_pred_classes = np.argmax(Y_pred, axis=1)
    Y_val_classes = np.argmax(Y_val, axis=1)

    # 计算精确率，召回率和 F1 分数
    precision = precision_score(Y_val_classes, Y_pred_classes)
    recall = recall_score(Y_val_classes, Y_pred_classes)
    f1 = f1_score(Y_val_classes, Y_pred_classes)

    print(f'For {feature}:\n Precision: {precision}, Recall: {recall}, F1 Score: {f1}, Accuracy: {acc}\n')

    # 将这次运行的结果存入字典
    results[feature] = {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'accuracy': acc
    }

# 转换 results 字典为 DataFrame，并保存到 CSV 文件
results_df = pd.DataFrame(results).T
results_df.to_csv('lstm_results.csv')



"""# W2V + LSTM"""



import pandas as pd
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess

# 假设df是你的DataFrame
df = email_data

# 对'subject'和'body'列进行预处理
subject_lines = [simple_preprocess(line) for line in df['subject'].tolist()]
body_lines = [simple_preprocess(line) for line in df['body'].tolist()]

# 合并两个列的预处理结果
lines = subject_lines + body_lines

# 实例化Word2Vec模型
model = Word2Vec(sentences=lines, vector_size=128, window=5, min_count=1, workers=4)

# 保存模型
model.save("my_word2vec.model")

"""bert是预训练模型
w2v预训练向量
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_score, recall_score, f1_score
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, SpatialDropout1D, LSTM, Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping
import matplotlib.pyplot as plt
from gensim.models import Word2Vec
from gensim.models import KeyedVectors

# 加载数据
df = email_data
# 设定词汇表的大小
num_words = 5000
# 设定每封邮件的最大长度
max_len = 500

# 初始化一个字典来存储每次运行的结果
results = {}

features = ['structure', 'subject', 'body', 'combined_strsub', 'combined_strbod', 'combined_subbod', 'combined']

for feature in features:
    print(f'Running for content: {feature}')

    df['content'] = df[feature]

    # 然后将分类标签转换为二进制格式
    encoder = LabelEncoder()
    df['spam'] = encoder.fit_transform(df['spam'])

    tokenizer = Tokenizer(num_words=num_words, split=' ')
    tokenizer.fit_on_texts(df['content'].values)

    X = tokenizer.texts_to_sequences(df['content'].values)
    X = pad_sequences(X, maxlen=max_len)
    Y = pd.get_dummies(df['spam']).values

    # 使用 Gensim 加载预训练的 Word2Vec 模型
    # 加载预训练的 Word2Vec 模型，例如Google的预训练模型
    # word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
    word2vec_model = Word2Vec.load('my_word2vec.model')
    embedding_matrix = np.zeros((num_words, 128))  # 假设你的 Word2Vec 模型的向量长度为 128
    for word, i in tokenizer.word_index.items():
        if i >= num_words:
            continue
        if word in word2vec_model.wv:
            embedding_matrix[i] = word2vec_model.wv[word]

    model = Sequential()
    model.add(Embedding(num_words, 128, input_length=max_len, weights=[embedding_matrix], trainable=False))
    model.add(SpatialDropout1D(0.4))  # 在 LSTM 层之前使用 Dropout
    model.add(LSTM(196, dropout=0.2, recurrent_dropout=0))  # 设置 recurrent_dropout=0
    model.add(Dense(2, activation='softmax'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)

    # 创建 ModelCheckpoint 和 EarlyStopping 回调函数
    checkpoint = ModelCheckpoint(f'lstm_w2v_{feature}_best_model.h5', verbose=1, monitor='val_loss', save_best_only=True,
                                 mode='auto')
    earlystop = EarlyStopping(monitor='val_loss', patience=3)

    # 在 fit 函数中使用这些回调函数
    history = model.fit(X_train, Y_train, epochs=10, batch_size=32, verbose=1, validation_data=(X_val, Y_val),
                        callbacks=[checkpoint, earlystop])
    # 绘制训练损失和验证损失
    plt.figure(figsize=(12,6))
    plt.subplot(1, 2, 1)
    plt.plot(history.history["loss"],label="Train loss")
    plt.plot(history.history["val_loss"],label="Test loss", linestyle='--')
    plt.title('Loss')
    plt.xlabel('epoch')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history["accuracy"],label="Train accuracy")
    plt.plot(history.history["val_accuracy"],label="Test accuracy", linestyle='--')
    plt.title('Accuracy')
    plt.xlabel('epoch')
    plt.legend()
    plt.show()

    # 计算在验证集上的评分和精确度
    score, acc = model.evaluate(X_val, Y_val, verbose=1, batch_size=32)

    # 在验证集上进行预测
    Y_pred = model.predict(X_val)

    # 将预测结果转换为类别
    Y_pred_classes = np.argmax(Y_pred, axis=1)
    Y_val_classes = np.argmax(Y_val, axis=1)

    # 计算精确率，召回率和 F1 分数
    precision = precision_score(Y_val_classes, Y_pred_classes)
    recall = recall_score(Y_val_classes, Y_pred_classes)
    f1 = f1_score(Y_val_classes, Y_pred_classes)

    print(f'For {feature}:\n Precision: {precision}, Recall: {recall}, F1 Score: {f1}, Accuracy: {acc}\n')

    # 将这次运行的结果存入字典
    results[feature] = {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'accuracy': acc
    }

# 转换 results 字典为 DataFrame，并保存到 CSV 文件
results_df = pd.DataFrame(results).T
results_df.to_csv('lstm_w2v_results.csv')

"""# BERT+LSTM"""

!pip install transformers

import os
import pickle
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_score, recall_score, f1_score
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping
import matplotlib.pyplot as plt
from transformers import DistilBertTokenizer, TFDistilBertModel
import tensorflow as tf

# 加载预训练模型/分词器
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')

def save_features(df, feature, path):
    # 如果已经保存了特征，直接加载它们
    if os.path.exists(path):
        with open(path, 'rb') as f:
            df['content_features'] = pickle.load(f)
    else:
        df['content_features'] = df[feature].apply(extract_features)
        # 保存特征以备将来使用
        with open(path, 'wb') as f:
            pickle.dump(df['content_features'].values, f)

def extract_features(text):
    input_ids = tf.constant(tokenizer.encode(text))[None, :]  # Batch size 1
    outputs = bert_model(input_ids)
    last_hidden_states = outputs[0]  # 最后隐藏状态是输出元组的第一个元素
    return last_hidden_states[0,0,:].numpy()  # 获取 [CLS] 标记的嵌入表示

# 加载数据
df = email_data

results = {}

features = ['body', 'combined_strsub', 'combined_strbod', 'combined_subbod', 'combined']

for feature in features:
    print(f'Running for content: {feature}')
    save_features(df, feature, f'{feature}_features.pkl')

    # 将标签转换为二进制格式
    encoder = LabelEncoder()
    df['spam'] = encoder.fit_transform(df['spam'])

    # 准备数据
    X = np.array(df['content_features'].to_list())
    Y = pd.get_dummies(df['spam']).values

    # 将数据拆分为训练集和验证集
    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)

    # 定义模型
    model = Sequential()
    model.add(LSTM(196, dropout=0.2, recurrent_dropout=0, input_shape=(None, 768)))  # 调整LSTM的输入形状
    model.add(Dense(2, activation='softmax'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # 设置回调函数
    checkpoint = ModelCheckpoint(f'lstm_bert_{feature}_best_model.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='auto')
    earlystop = EarlyStopping(monitor='val_loss', patience=3)

    # 训练模型
    history = model.fit(X_train[:, None], Y_train, epochs=10, batch_size=32, verbose=1, validation_data=(X_val[:, None], Y_val), callbacks=[checkpoint, earlystop])

    # 绘制训练历史
    plt.figure(figsize=(12,6))
    plt.subplot(1, 2, 1)
    plt.plot(history.history["loss"],label="Train loss")
    plt.plot(history.history["val_loss"],label="Test loss", linestyle='--')
    plt.title('Loss')
    plt.xlabel('epoch')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history["accuracy"],label="Train accuracy")
    plt.plot(history.history["val_accuracy"],label="Test accuracy", linestyle='--')
    plt.title('Accuracy')
    plt.xlabel('epoch')
    plt.legend()
    plt.show()

    # 评估模型
    score, acc = model.evaluate(X_val[:, None], Y_val, verbose=1, batch_size=32)

    # 在验证集上进行预测
    Y_pred = model.predict(X_val[:, None])

    # 将预测转换为类别
    Y_pred_classes = np.argmax(Y_pred, axis=1)
    Y_val_classes = np.argmax(Y_val, axis=1)

    # 计算精确度、召回率和F1得分
    precision = precision_score(Y_val_classes, Y_pred_classes)
    recall = recall_score(Y_val_classes, Y_pred_classes)
    f1 = f1_score(Y_val_classes, Y_pred_classes)

    print(f'For {feature}:\n Precision: {precision}, Recall: {recall}, F1 Score: {f1}, Accuracy: {acc}\n')
    # 将这次运行的结果存入字典
    results[feature] = {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'accuracy': acc
    }


# 转换 results 字典为 DataFrame，并保存到 CSV 文件
results_df = pd.DataFrame(results).T
results_df.to_csv('lstm_bert_results.csv')

import os
import pickle
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_score, recall_score, f1_score
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping
import matplotlib.pyplot as plt
from transformers import DistilBertTokenizer, TFDistilBertModel
import tensorflow as tf

# 加载预训练模型/分词器
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')

def save_features(df, feature, path):
    # 如果已经保存了特征，直接加载它们
    if os.path.exists(path):
        with open(path, 'rb') as f:
            df['content_features'] = pickle.load(f)
    else:
        df['content_features'] = df[feature].apply(extract_features)
        # 保存特征以备将来使用
        with open(path, 'wb') as f:
            pickle.dump(df['content_features'].values, f)

def extract_features(text):
    input_ids = tf.constant(tokenizer.encode(text, max_length=512, truncation=True))[None, :]  # Batch size 1
    outputs = bert_model(input_ids)
    last_hidden_states = outputs[0]  # 最后隐藏状态是输出元组的第一个元素
    return last_hidden_states[0,0,:].numpy()  # 获取 [CLS] 标记的嵌入表示


# 加载数据
df = email_data

results = {}

features = [ 'body', 'combined_strsub', 'combined_strbod', 'combined_subbod', 'combined']

for feature in features:
    print(f'Running for content: {feature}')
    save_features(df, feature, f'{feature}_features.pkl')

    # 将标签转换为二进制格式
    encoder = LabelEncoder()
    df['spam'] = encoder.fit_transform(df['spam'])

    # 准备数据
    X = np.array(df['content_features'].to_list())
    Y = pd.get_dummies(df['spam']).values

    # 将数据拆分为训练集和验证集
    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)

    # 定义模型
    model = Sequential()
    model.add(LSTM(196, dropout=0.2, recurrent_dropout=0, input_shape=(None, 768)))  # 调整LSTM的输入形状
    model.add(Dense(2, activation='softmax'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # 设置回调函数
    checkpoint = ModelCheckpoint(f'lstm_bert_{feature}_best_model.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='auto')
    earlystop = EarlyStopping(monitor='val_loss', patience=3)

    # 训练模型
    history = model.fit(X_train[:, None], Y_train, epochs=10, batch_size=32, verbose=1, validation_data=(X_val[:, None], Y_val), callbacks=[checkpoint, earlystop])

    # 绘制训练历史
    plt.figure(figsize=(12,6))
    plt.subplot(1, 2, 1)
    plt.plot(history.history["loss"],label="Train loss")
    plt.plot(history.history["val_loss"],label="Test loss", linestyle='--')
    plt.title('Loss')
    plt.xlabel('epoch')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history["accuracy"],label="Train accuracy")
    plt.plot(history.history["val_accuracy"],label="Test accuracy", linestyle='--')
    plt.title('Accuracy')
    plt.xlabel('epoch')
    plt.legend()
    plt.show()

    # 评估模型
    score, acc = model.evaluate(X_val[:, None], Y_val, verbose=1, batch_size=32)

    # 在验证集上进行预测
    Y_pred = model.predict(X_val[:, None])

    # 将预测转换为类别
    Y_pred_classes = np.argmax(Y_pred, axis=1)
    Y_val_classes = np.argmax(Y_val, axis=1)

    # 计算精确度、召回率和F1得分
    precision = precision_score(Y_val_classes, Y_pred_classes)
    recall = recall_score(Y_val_classes, Y_pred_classes)
    f1 = f1_score(Y_val_classes, Y_pred_classes)

    print(f'For {feature}:\n Precision: {precision}, Recall: {recall}, F1 Score: {f1}, Accuracy: {acc}\n')
    # 将这次运行的结果存入字典
    results[feature] = {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'accuracy': acc
    }


# 转换 results 字典为 DataFrame，并保存到 CSV 文件
results_df = pd.DataFrame(results).T
results_df.to_csv('lstm_bert_results.csv')

import os
import pickle
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_score, recall_score, f1_score
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping
import matplotlib.pyplot as plt
from transformers import DistilBertTokenizer, TFDistilBertModel
import tensorflow as tf

# 加载预训练模型/分词器
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')

def save_features(df, feature, path):
    # 如果已经保存了特征，直接加载它们
    if os.path.exists(path):
        with open(path, 'rb') as f:
            df['content_features'] = pickle.load(f)
    else:
        df['content_features'] = df[feature].apply(extract_features)
        # 保存特征以备将来使用
        with open(path, 'wb') as f:
            pickle.dump(df['content_features'].values, f)

def extract_features(text):
    input_ids = tf.constant(tokenizer.encode(text, max_length=512, truncation=True))[None, :]  # Batch size 1
    outputs = bert_model(input_ids)
    last_hidden_states = outputs[0]  # 最后隐藏状态是输出元组的第一个元素
    return last_hidden_states[0,0,:].numpy()  # 获取 [CLS] 标记的嵌入表示

# 加载数据
df = email_data

results = {}

features = ['combined']

for feature in features:
    print(f'Running for content: {feature}')
    save_features(df, feature, f'{feature}_features.pkl')

    # 将标签转换为二进制格式
    encoder = LabelEncoder()
    df['spam'] = encoder.fit_transform(df['spam'])

    # 准备数据
    X = np.array(df['content_features'].to_list())
    Y = pd.get_dummies(df['spam']).values

    # 将数据拆分为训练集和验证集
    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)

    # 定义模型
    model = Sequential()
    model.add(LSTM(196, dropout=0.2, recurrent_dropout=0, input_shape=(None, 768)))  # 调整LSTM的输入形状
    model.add(Dense(2, activation='softmax'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # 设置回调函数
    checkpoint = ModelCheckpoint(f'lstm_bert_{feature}_best_model.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='auto')
    earlystop = EarlyStopping(monitor='val_loss', patience=3)

    # 训练模型
    history = model.fit(X_train[:, None], Y_train, epochs=10, batch_size=32, verbose=1, validation_data=(X_val[:, None], Y_val), callbacks=[checkpoint, earlystop])

    # 绘制训练历史
    plt.figure(figsize=(12,6))
    plt.subplot(1, 2, 1)
    plt.plot(history.history["loss"],label="Train loss")
    plt.plot(history.history["val_loss"],label="Test loss", linestyle='--')
    plt.title('Loss')
    plt.xlabel('epoch')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history["accuracy"],label="Train accuracy")
    plt.plot(history.history["val_accuracy"],label="Test accuracy", linestyle='--')
    plt.title('Accuracy')
    plt.xlabel('epoch')
    plt.legend()
    plt.show()

    # 评估模型
    score, acc = model.evaluate(X_val[:, None], Y_val, verbose=1, batch_size=32)

    # 在验证集上进行预测
    Y_pred = model.predict(X_val[:, None])

    # 将预测转换为类别
    Y_pred_classes = np.argmax(Y_pred, axis=1)
    Y_val_classes = np.argmax(Y_val, axis=1)

    # 计算精确度、召回率和F1得分
    precision = precision_score(Y_val_classes, Y_pred_classes)
    recall = recall_score(Y_val_classes, Y_pred_classes)
    f1 = f1_score(Y_val_classes, Y_pred_classes)

    print(f'For {feature}:\n Precision: {precision}, Recall: {recall}, F1 Score: {f1}, Accuracy: {acc}\n')
    # 将这次运行的结果存入字典
    results[feature] = {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'accuracy': acc
    }


# 转换 results 字典为 DataFrame，并保存到 CSV 文件
results_df = pd.DataFrame(results).T
results_df.to_csv('lstm_bert_results.csv')