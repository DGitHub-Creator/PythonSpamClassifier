# -*- coding: utf-8 -*-
"""1DCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c12JOUt0OSaLTtZu0-EKrE7Sv-S4SL_8
"""

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')
import os
path = "/content/drive/MyDrive/"
os.chdir(path)

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, LSTM, SpatialDropout1D
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.models import load_model
import matplotlib.pyplot as plt
import csv

csv.field_size_limit(2**25)  # 提高字段大小限制
email_data = pd.read_csv("cleaned_email_06_full.csv", dtype={'email_id': object}, engine='python')
print(email_data.info())
labels = pd.get_dummies(email_data["spam"]).values
email_data['structure'] = email_data['parts'].astype(str) + email_data['attachments'].astype(str) + email_data['html'].astype(str) + email_data['links'].astype(str)
email_data['combined_strsub'] = email_data['structure'] + email_data['subject']
email_data['combined_strbod'] = email_data['structure'] + email_data['body']
email_data['combined_subbod'] = email_data['subject'] + email_data['body']
email_data['combined'] = email_data['structure'] + email_data['subject'] + email_data['body']
email_data.info()

# 加载数据
df = email_data  # 请替换为你的数据文件路径

# 设定词汇表的大小
num_words = 5000
# 设定每封邮件的最大长度（如果邮件超过此长度，则将其截断，如果不足，则用0填充）
max_len = 500

# 初始化一个字典来存储每次运行的结果
results = {}

features = ['structure', 'subject', 'body', 'combined_strsub', 'combined_strbod', 'combined_subbod', 'combined']

for feature in features:
    print(f'Running for content: {feature}')

    df['content'] = df[feature]

    # 然后将分类标签转换为二进制格式
    encoder = LabelEncoder()
    df['spam'] = encoder.fit_transform(df['spam'])

    tokenizer = Tokenizer(num_words=num_words, split=' ')
    tokenizer.fit_on_texts(df['content'].values)

    X = tokenizer.texts_to_sequences(df['content'].values)
    X = pad_sequences(X, maxlen=max_len)
    Y = pd.get_dummies(df['spam']).values

    model = Sequential()
    model.add(Embedding(num_words, 128, input_length=max_len))
    model.add(Conv1D(128, 5, activation='relu'))
    model.add(GlobalMaxPooling1D())
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(2, activation='softmax'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)

    # 创建 ModelCheckpoint 和 EarlyStopping 回调函数
    checkpoint = ModelCheckpoint(f'1dcnn_{feature}_best_model.h5', verbose=1, monitor='val_loss', save_best_only=True,
                                 mode='auto')
    earlystop = EarlyStopping(monitor='val_loss', patience=3)

    # 在 fit 函数中使用这些回调函数
    history = model.fit(X_train, Y_train, epochs=10, batch_size=32, verbose=1, validation_data=(X_val, Y_val),
                        callbacks=[checkpoint, earlystop])

    # 绘制训练损失和验证损失
    plt.figure(figsize=(12,6))
    plt.subplot(1, 2, 1)
    plt.plot(history.history["loss"],label="Train loss")
    plt.plot(history.history["val_loss"],label="Test loss", linestyle='--')
    plt.title('Loss')
    plt.xlabel('epoch')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history["accuracy"],label="Train accuracy")
    plt.plot(history.history["val_accuracy"],label="Test accuracy", linestyle='--')
    plt.title('Accuracy')
    plt.xlabel('epoch')
    plt.legend()
    plt.show()

    # 在验证集上进行预测
    Y_pred = model.predict(X_val)

    # 将预测结果转换为类别
    Y_pred_classes = np.argmax(Y_pred, axis=1)
    Y_val_classes = np.argmax(Y_val, axis=1)

    # 计算精确率，召回率和 F1 分数
    precision = precision_score(Y_val_classes, Y_pred_classes)
    recall = recall_score(Y_val_classes, Y_pred_classes)
    f1 = f1_score(Y_val_classes, Y_pred_classes)

    # 在 fit 过程中已经记录了最佳模型，无需再重新加载
    best_model_path = f'1dcnn_{feature}_best_model.h5'
    best_model = load_model(best_model_path)

    # 在验证集上评估最佳模型
    score, acc = best_model.evaluate(X_val, Y_val, verbose=1, batch_size=32)

    print(f'For {feature}:\n Precision: {precision}, Recall: {recall}, F1 Score: {f1}, Accuracy: {acc}\n')

    # 将这次运行的结果存入字典
    results[feature] = {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'accuracy': acc
    }

# 转换 results 字典为 DataFrame，并保存到 CSV 文件
results_df = pd.DataFrame(results).T
results_df.to_csv('1dcnn_results.csv')