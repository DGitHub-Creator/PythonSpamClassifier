# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FX_n4s-eg6yVyMCdeFnINtN_QPFRp17W
"""

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')
import os
path = "/content/drive/MyDrive/"
os.chdir(path)

!pip install transformers
!pip install torch
!pip install pandas

# Standard library imports
import csv
import torch
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from torch.cuda.amp import GradScaler, autocast
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW

csv.field_size_limit(2**25)  # 提高字段大小限制
email_data = pd.read_csv("cleaned_email_06_full.csv", dtype={'email_id': object}, engine='python')
print(email_data.info())
labels = pd.get_dummies(email_data["spam"]).values
email_data['structure'] = email_data['parts'].astype(str) + email_data['attachments'].astype(str) + email_data['html'].astype(str) + email_data['links'].astype(str)
email_data['combined_strsub'] = email_data['structure'] + email_data['subject']
email_data['combined_strbod'] = email_data['structure'] + email_data['body']
email_data['combined_subbod'] = email_data['subject'] + email_data['body']
email_data['combined'] = email_data['structure'] + email_data['subject'] + email_data['body']
email_data.info()





"""# Only BERT"""



class SpamDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        return {
            'text': text,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

def train_model(X, y, model, optimizer, model_save_path):
    # 分割数据集
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    # 创建数据集
    train_dataset = SpamDataset(X_train.tolist(), y_train.tolist(), tokenizer, MAX_LEN)
    val_dataset = SpamDataset(X_val.tolist(), y_val.tolist(), tokenizer, MAX_LEN)

    train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_data_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    best_loss = None
    patience = 0

    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        for step, batch in enumerate(tqdm(train_data_loader, desc="Training")):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            with autocast():  # 在 autocast 上下文中执行前向传播
                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS
            scaler.scale(loss).backward()
            if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:  # 只有在每个累积步骤完成后才更新模型参数和优化器
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
            total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS
        avg_train_loss = total_loss / len(train_data_loader)
        print(f'Training loss: {avg_train_loss}')

        # 验证模型
        model.eval()
        total_loss = 0
        predictions, true_labels = [], []
        for batch in tqdm(val_data_loader, desc="Validating"):  # 添加进度条
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            with torch.no_grad():
                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            logits = outputs.logits
            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1).flatten()
            predictions.extend(preds.cpu().numpy().tolist())
            true_labels.extend(labels.cpu().numpy().tolist())
        avg_val_loss = total_loss / len(val_data_loader)
        print(f'Validation loss: {avg_val_loss}')

        # 保存最佳模型
        if best_loss is None or avg_val_loss < best_loss:
            torch.save(model.state_dict(), model_save_path)
            best_loss = avg_val_loss
            patience = 0
        else:
            patience += 1
        if patience == PATIENCE_LIMIT:
            break

    report = classification_report(true_labels, predictions, target_names=['Not Spam', 'Spam'], digits=10)
    print(report)


# 使用DistilBert
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# 最大序列长度
MAX_LEN = 256

BATCH_SIZE = 32

GRADIENT_ACCUMULATION_STEPS = 8

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

EPOCHS = 3

PATIENCE_LIMIT = 2

scaler = GradScaler()

df = email_data
# 定义所有的输入
inputs = ['structure', 'subject', 'body', 'combined_strsub', 'combined_strbod', 'combined_subbod', 'combined']

for i, X in enumerate(inputs):
    print(f'Training model for input: {X}')
    model_save_path = f'model_{i}_best.pt'
    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2).to(device)
    optimizer = AdamW(model.parameters(), lr=2e-5)
    train_model(df[X], df['spam'], model, optimizer, model_save_path)
    # del model  # 删除旧的模型实例
    torch.cuda.empty_cache()  # 清空未使用的 GPU 内存

"""# 失败

# BERT and LSTM
"""



del model
del optimizer
torch.cuda.empty_cache()  # 清空未使用的 GPU 内存

import torch.nn as nn
from transformers import DistilBertModel

class BertLSTM(nn.Module):
    def __init__(self, bert_model, num_labels, hidden_size, num_layers, bidirectional, dropout):
        super(BertLSTM, self).__init__()
        self.bert_model = bert_model
        self.lstm = nn.LSTM(bert_model.config.hidden_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)
        self.classifier = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, num_labels)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.bert_model(input_ids, attention_mask=attention_mask)
        _, pooled_output = self.lstm(outputs.last_hidden_state)
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
        return loss, logits

class SpamDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        return {
            'text': text,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

def train_model(X, y, model, optimizer, model_save_path):
    # 分割数据集
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    # 创建数据集
    train_dataset = SpamDataset(X_train.tolist(), y_train.tolist(), tokenizer, MAX_LEN)
    val_dataset = SpamDataset(X_val.tolist(), y_val.tolist(), tokenizer, MAX_LEN)

    train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_data_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    best_loss = None
    patience = 0

    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        for step, batch in enumerate(tqdm(train_data_loader, desc="Training")):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            with autocast():  # 在 autocast 上下文中执行前向传播
                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS
            scaler.scale(loss).backward()
            if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:  # 只有在每个累积步骤完成后才更新模型参数和优化器
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
            total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS
        avg_train_loss = total_loss / len(train_data_loader)
        print(f'Training loss: {avg_train_loss}')

        # 验证模型
        model.eval()
        total_loss = 0
        predictions, true_labels = [], []
        for batch in tqdm(val_data_loader, desc="Validating"):  # 添加进度条
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            with torch.no_grad():
                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            logits = outputs.logits
            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1).flatten()
            predictions.extend(preds.cpu().numpy().tolist())
            true_labels.extend(labels.cpu().numpy().tolist())
        avg_val_loss = total_loss / len(val_data_loader)
        print(f'Validation loss: {avg_val_loss}')

        # 保存最佳模型
        if best_loss is None or avg_val_loss < best_loss:
            torch.save(model.state_dict(), model_save_path)
            best_loss = avg_val_loss
            patience = 0
        else:
            patience += 1
        if patience == PATIENCE_LIMIT:
            break

    report = classification_report(true_labels, predictions, target_names=['Not Spam', 'Spam'], digits=10)
    print(report)


# 使用DistilBert
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# 最大序列长度
MAX_LEN = 256

BATCH_SIZE = 32

GRADIENT_ACCUMULATION_STEPS = 8

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

EPOCHS = 3

PATIENCE_LIMIT = 2

scaler = GradScaler()

df = email_data
# 定义所有的输入
inputs = ['structure', 'subject', 'body', 'combined_strbod', 'combined_subbod', 'combined_strsub', 'combined']

# 建立模型
HIDDEN_SIZE = 128
NUM_LAYERS = 2
BIDIRECTIONAL = True
DROPOUT = 0.1

for i, X in enumerate(inputs):
    print(f'Training model for input: {X}')
    model_save_path = f'model_{i}_best_lstm.pt'
    bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)
    model = BertLSTM(bert_model, num_labels=2, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, bidirectional=BIDIRECTIONAL, dropout=DROPOUT).to(device)
    optimizer = AdamW(model.parameters(), lr=2e-5)
    train_model(df[X], df['spam'], model, optimizer, model_save_path)
    # del model  # 删除旧的模型实例
    torch.cuda.empty_cache()  # 清空未使用的 GPU 内存



del model
del optimizer
torch.cuda.empty_cache()  # 清空未使用的 GPU 内存

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import DistilBertTokenizer, DistilBertModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tqdm import tqdm

class SpamDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        return {
            'text': text,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class BERTLSTMClassifier(nn.Module):
    def __init__(self, bert_model, lstm_hidden_size, num_labels):
        super(BERTLSTMClassifier, self).__init__()
        self.bert = bert_model
        self.lstm = nn.LSTM(
            input_size=bert_model.config.hidden_size,
            hidden_size=lstm_hidden_size,
            num_layers=1,
            batch_first=True,
            bidirectional=False
        )
        self.dropout = nn.Dropout(p=0.2)
        self.linear = nn.Linear(lstm_hidden_size, num_labels)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        last_hidden_state = outputs.last_hidden_state
        pooled_output = last_hidden_state[:, 0, :]
        lstm_input = pooled_output.unsqueeze(0)
        lstm_output, _ = self.lstm(lstm_input)
        lstm_output = self.dropout(lstm_output)
        logits = self.linear(lstm_output.squeeze(0))
        return logits



def train_model(X, y, model, optimizer, model_save_path):
    # 分割数据集
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    # 创建数据集
    train_dataset = SpamDataset(X_train.tolist(), y_train.tolist(), tokenizer, MAX_LEN)
    val_dataset = SpamDataset(X_val.tolist(), y_val.tolist(), tokenizer, MAX_LEN)

    train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_data_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    best_loss = None
    patience = 0

    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        for step, batch in enumerate(tqdm(train_data_loader, desc="Training")):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()

            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)

            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_data_loader)
        print(f'Training loss: {avg_train_loss}')

        # 验证模型
        model.eval()
        total_loss = 0
        predictions, true_labels = [], []
        with torch.no_grad():
            for batch in tqdm(val_data_loader, desc="Validating"):
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                logits = model(input_ids, attention_mask)
                loss = criterion(logits, labels)

                total_loss += loss.item()

                preds = torch.argmax(logits, dim=1).flatten()
                predictions.extend(preds.cpu().numpy().tolist())
                true_labels.extend(labels.cpu().numpy().tolist())

        avg_val_loss = total_loss / len(val_data_loader)
        print(f'Validation loss: {avg_val_loss}')

        # 保存最佳模型
        if best_loss is None or avg_val_loss < best_loss:
            torch.save(model.state_dict(), model_save_path)
            best_loss = avg_val_loss
            patience = 0
        else:
            patience += 1
        if patience == PATIENCE_LIMIT:
            break

    report = classification_report(true_labels, predictions, target_names=['Not Spam', 'Spam'], digits=10)
    print(report)

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

MAX_LEN = 256

BATCH_SIZE = 32

GRADIENT_ACCUMULATION_STEPS = 8

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

EPOCHS = 3

PATIENCE_LIMIT = 2

scaler = torch.cuda.amp.GradScaler()

df = email_data

inputs = ['structure', 'subject', 'body', 'combined_strsub', 'combined_strbod', 'combined_subbod', 'combined']

for i, X in enumerate(inputs):
  print(f'Training model for input: {X}')
  model_save_path = f'model_{i}_best.pt'
  # 初始化BERT模型和LSTM分类器
  bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')
  lstm_hidden_size = 256
  num_labels = 2
  model = BERTLSTMClassifier(bert_model, lstm_hidden_size, num_labels).to(device)

  optimizer = optim.AdamW(model.parameters(), lr=2e-5)
  criterion = nn.CrossEntropyLoss()

  train_model(df[X], df['spam'], model, optimizer, model_save_path)

  # del model
  torch.cuda.empty_cache()